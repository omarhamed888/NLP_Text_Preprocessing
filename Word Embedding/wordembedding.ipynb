{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embedding\n",
    "Provide an overview of word embedding and its importance in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Citations\n",
    "List key citations and references for further reading on word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Word Embedding\n",
    "\n",
    "Word embedding is a technique used in natural language processing (NLP) to represent words in a continuous vector space. This allows words with similar meanings to have similar representations, which can be used to improve the performance of various NLP tasks such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "### Importance of Word Embedding\n",
    "\n",
    "1. **Captures Semantic Relationships**: Word embeddings capture semantic relationships between words, allowing models to understand context and meaning.\n",
    "2. **Dimensionality Reduction**: By representing words as vectors, word embeddings reduce the dimensionality of text data, making it more manageable for machine learning algorithms.\n",
    "3. **Improves Model Performance**: Using word embeddings can significantly improve the performance of NLP models by providing them with richer and more informative input data.\n",
    "\n",
    "### Key Citations\n",
    "\n",
    "- [Word Embedding - Wikipedia](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "- [Word embeddings in NLP: A Complete Guide](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/)\n",
    "- [Word Embeddings in NLP - GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/)\n",
    "- [What Are Word Embeddings? | IBM](https://www.ibm.com/cloud/learn/word-embeddings)\n",
    "- [What Are Word Embeddings for Text? - MachineLearningMastery.com](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "- [The Ultimate Guide to Word Embeddings](https://towardsdatascience.com/the-ultimate-guide-to-word-embeddings-in-nlp-5cdd0f1d5e8c)\n",
    "- [An intuitive introduction to text embeddings - Stack Overflow](https://stackoverflow.blog/2020/07/27/a-quick-introduction-to-text-embeddings/)\n",
    "- [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/)\n",
    "- [What are Word Embeddings? | A Comprehensive Word Embedding Guide | Elastic](https://www.elastic.co/blog/what-are-word-embeddings)\n",
    "- [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)\n",
    "- [Word2vec - Wikipedia](https://en.wikipedia.org/wiki/Word2vec)\n",
    "- [A Dummyâ€™s Guide to Word2Vec - Medium](https://medium.com/@mishra.thedeepak/a-dummys-guide-to-word2vec-2b865a8339e0)\n",
    "- [Word2Vec: Explanation and Examples - Serokell](https://serokell.io/blog/word2vec)\n",
    "- [Word Embedding using Word2Vec - GeeksforGeeks](https://www.geeksforgeeks.org/word-embedding-using-word2vec/)\n",
    "- [word2vec | Text | TensorFlow](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "- [Word2Vec For Word Embeddings -A Beginner's Guide - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/)\n",
    "- [Practice Word2Vec for NLP Using Python | Built In](https://builtin.com/data-science/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including Gensim and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Word2Vec\n",
    "Explain the Word2Vec model and its components, including CBOW and Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data\n",
    "text = \"Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space.\"\n",
    "\n",
    "# Preprocess the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(text.lower())\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Create CBOW model\n",
    "cbow_model = Word2Vec([filtered_words], vector_size=50, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Create Skip-gram model\n",
    "skipgram_model = Word2Vec([filtered_words], vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Display the vector for a word\n",
    "print(\"Vector for 'word' using CBOW:\", cbow_model.wv['word'])\n",
    "print(\"Vector for 'word' using Skip-gram:\", skipgram_model.wv['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Word2Vec with Gensim\n",
    "Provide code examples to implement Word2Vec using the Gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data\n",
    "text = \"Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space.\"\n",
    "\n",
    "# Preprocess the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(text.lower())\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Create CBOW model\n",
    "cbow_model = Word2Vec([filtered_words], vector_size=50, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Create Skip-gram model\n",
    "skipgram_model = Word2Vec([filtered_words], vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Display the vector for a word\n",
    "print(\"Vector for 'word' using CBOW:\", cbow_model.wv['word'])\n",
    "print(\"Vector for 'word' using Skip-gram:\", skipgram_model.wv['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Word Embeddings\n",
    "Use Matplotlib to visualize the word embeddings in a 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Word Embeddings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get the word vectors from the CBOW model\n",
    "word_vectors = cbow_model.wv\n",
    "\n",
    "# Reduce the dimensionality of the word vectors to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_2d = pca.fit_transform(word_vectors.vectors)\n",
    "\n",
    "# Create a scatter plot of the word vectors\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
    "\n",
    "# Annotate the points with the corresponding words\n",
    "for i, word in enumerate(word_vectors.index_to_key):\n",
    "    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
    "\n",
    "plt.title('2D Visualization of Word Embeddings (CBOW)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Word Embeddings\n",
    "Discuss various applications of word embeddings in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample text data for classification\n",
    "texts = [\"I love this movie\", \"I hate this movie\", \"This film is great\", \"This film is terrible\"]\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    return [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "processed_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "# Get the average word vectors for each text\n",
    "def get_average_vector(words, model):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X = np.array([get_average_vector(text, cbow_model) for text in processed_texts])\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
