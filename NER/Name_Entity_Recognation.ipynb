{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including spaCy, pandas, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import spacy  # spaCy for NLP tasks\n",
    "import pandas as pd  # pandas for data manipulation\n",
    "from sklearn.model_selection import train_test_split  # scikit-learn for splitting data into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "Load a dataset containing text data with named entity annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Preprocess the text data, including tokenization and annotation formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# Tokenization and annotation formatting\n",
    "def preprocess_data(df):\n",
    "    # Tokenize the text data\n",
    "    df['Tokens'] = df['Text'].apply(lambda x: [token.text for token in nlp(x)])\n",
    "    \n",
    "    # Format annotations\n",
    "    annotations = []\n",
    "    for _, row in df.iterrows():\n",
    "        entities = []\n",
    "        for start, end, label in zip(row['Start'], row['End'], row['Label']):\n",
    "            entities.append((start, end, label))\n",
    "        annotations.append({'entities': entities})\n",
    "    \n",
    "    df['Annotations'] = annotations\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing to the DataFrame\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Display the first few rows of the preprocessed DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split\n",
    "Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the number of samples in the training and testing sets\n",
    "print(f\"Number of training samples: {len(train_df)}\")\n",
    "print(f\"Number of testing samples: {len(test_df)}\")\n",
    "\n",
    "# Display the first few rows of the training DataFrame\n",
    "train_df.head()\n",
    "\n",
    "# Display the first few rows of the testing DataFrame\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NER Model\n",
    "Build and train a Named Entity Recognition model using spaCy or another NLP library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NER Model\n",
    "\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Create a blank NER model\n",
    "ner_model = spacy.blank(\"en\")\n",
    "\n",
    "# Add the NER pipeline component\n",
    "ner = ner_model.create_pipe(\"ner\")\n",
    "ner_model.add_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the NER component\n",
    "for _, annotations in train_df['Annotations'].items():\n",
    "    for ent in annotations['entities']:\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Convert the training data to spaCy's Example format\n",
    "train_data = []\n",
    "for text, annotations in zip(train_df['Text'], train_df['Annotations']):\n",
    "    doc = ner_model.make_doc(text)\n",
    "    example = Example.from_dict(doc, {\"entities\": annotations['entities']})\n",
    "    train_data.append(example)\n",
    "\n",
    "# Train the NER model\n",
    "optimizer = ner_model.begin_training()\n",
    "for i in range(10):  # Number of training iterations\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        ner_model.update(batch, drop=0.5, losses=losses)\n",
    "    print(f\"Iteration {i+1}, Losses: {losses}\")\n",
    "\n",
    "# Save the trained model to disk\n",
    "ner_model.to_disk(\"ner_model\")\n",
    "\n",
    "# Load the trained model\n",
    "trained_ner_model = spacy.load(\"ner_model\")\n",
    "\n",
    "# Test the trained model on a sample text\n",
    "sample_text = test_df['Text'].iloc[0]\n",
    "doc = trained_ner_model(sample_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n",
    "Evaluate the performance of the NER model using appropriate metrics such as precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to convert spaCy doc to a list of tuples (start, end, label)\n",
    "def get_entities(doc):\n",
    "    return [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "true_entities = []\n",
    "pred_entities = []\n",
    "\n",
    "for text, annotations in zip(test_df['Text'], test_df['Annotations']):\n",
    "    doc = trained_ner_model(text)\n",
    "    true_entities.append(annotations['entities'])\n",
    "    pred_entities.append(get_entities(doc))\n",
    "\n",
    "# Flatten the lists\n",
    "true_entities_flat = [item for sublist in true_entities for item in sublist]\n",
    "pred_entities_flat = [item for sublist in pred_entities for item in sublist]\n",
    "\n",
    "# Extract the labels\n",
    "true_labels = [label for _, _, label in true_entities_flat]\n",
    "pred_labels = [label for _, _, label in pred_entities_flat]\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(true_labels, pred_labels, zero_division=0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results\n",
    "Visualize the results of the NER model on sample text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n",
    "\n",
    "# Function to visualize named entities in text\n",
    "def visualize_ner(text, model):\n",
    "    doc = model(text)\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "# Visualize the results of the NER model on sample text data\n",
    "sample_texts = test_df['Text'].head(5).tolist()\n",
    "\n",
    "for text in sample_texts:\n",
    "    visualize_ner(text, trained_ner_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
